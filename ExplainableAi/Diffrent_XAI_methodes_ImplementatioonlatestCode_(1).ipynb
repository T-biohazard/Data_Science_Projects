{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgXb1pE1RuBE",
        "outputId": "1e5ffc10-383f-417a-9e64-e297c1c1b5ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch-grad-cam (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch-grad-cam\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torchvision torch pytorch-grad-cam\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grad-cam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G3ALUFFs6nK",
        "outputId": "f35d114b-a217-4869-9f15-fc5102c6daa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: grad-cam in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from grad-cam) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from grad-cam) (10.4.0)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from grad-cam) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from grad-cam) (0.19.1+cu121)\n",
            "Requirement already satisfied: ttach in /usr/local/lib/python3.10/dist-packages (from grad-cam) (0.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from grad-cam) (4.66.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from grad-cam) (4.10.0.84)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from grad-cam) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from grad-cam) (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (2024.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.1->grad-cam) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.1->grad-cam) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_grad_cam import GradCAM # this should now work\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget"
      ],
      "metadata": {
        "id": "hmK4G1vPR1pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZgLDm8tr2IX",
        "outputId": "acc9d996-855d-4db1-fbeb-a62c135d5b81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your dataset (replace with your actual path in Google Drive)\n",
        "img_folder = '/content/drive/MyDrive/pocs/'\n",
        "infected_folder = img_folder + 'infected/'\n",
        "non_infected_folder = img_folder + 'notinfected/' #Fixed typo: changed 'non_infected' to 'non-infected'\n"
      ],
      "metadata": {
        "id": "tSjRvKver28o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "img_paths = []\n",
        "labels = []"
      ],
      "metadata": {
        "id": "gVrGYbLvr6G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have two folders 'infected' and 'non_infected'\n",
        "# Collect infected images\n",
        "for img_name in os.listdir(infected_folder):\n",
        "    img_paths.append(os.path.join(infected_folder, img_name))\n",
        "    labels.append(1)  # Label 1 for infected"
      ],
      "metadata": {
        "id": "KDA_CH3or6sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect non-infected images\n",
        "for img_name in os.listdir(non_infected_folder):\n",
        "    img_paths.append(os.path.join(non_infected_folder, img_name))\n",
        "    labels.append(0)  # Label 0 for non-infected"
      ],
      "metadata": {
        "id": "wtR9w-k1sJQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MedicalDataset class\n",
        "class MedicalDataset(Dataset):\n",
        "    def __init__(self, img_paths, labels, transform=None):\n",
        "        self.img_paths = img_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        image = Image.open(img_path).convert('RGB') # Ensure the image is loaded as RGB\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "EAZRVUjbR5OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n"
      ],
      "metadata": {
        "id": "Pe6jDs1esVgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset and dataloader\n",
        "dataset = MedicalDataset(img_paths, labels, transform=transform)\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "8jo_tTXbsbI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load DenseNet201 pre-trained model\n",
        "model = models.densenet201(pretrained=True)\n",
        "\n",
        "# Replace the classifier layer to fit your number of classes\n",
        "model.classifier = nn.Linear(1920, 2)  # Assuming binary classification (infected vs non-infected)\n",
        "\n",
        "# Move the model to the appropriate device (GPU/CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "yvwTV9DuR79i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d74f5ad-faf0-47a6-8b2c-93a18bbd7be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n",
            "100%|██████████| 77.4M/77.4M [00:00<00:00, 121MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs =25  # Adjust based on your dataset\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in data_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(data_loader)}')\n"
      ],
      "metadata": {
        "id": "TmTlRVvNSCzE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0db2f974-877b-4506-afbf-9f3b687f7933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/25], Loss: 0.010713125930469837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Grad-CAM\n",
        "target_layers = [model.features.denseblock4]  # Select the target layer to visualize\n",
        "\n",
        "# Remove use_cuda argument\n",
        "cam = GradCAM(model=model, target_layers=target_layers)"
      ],
      "metadata": {
        "id": "gpXOl4h7nUP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model on a new image\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1 (8).jpg'  # Your test image\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "pUrQMQvnuPw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate CAM\n",
        "targets = [ClassifierOutputTarget(1)]  # Assuming label 1 corresponds to 'infected'\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n"
      ],
      "metadata": {
        "id": "6pYbSQVJuRV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resize the original image to match the CAM dimensions\n",
        "img = img.resize((224, 224))\n",
        "\n",
        "# Convert the image to the correct data type and number of channels\n",
        "img_np = np.array(img)\n",
        "if len(img_np.shape) == 2:  # Grayscale image\n",
        "    img_np = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "# Normalize the image to be in the range [0, 255] and ensure it's uint8\n",
        "img_np = np.uint8(255 * img_np / np.max(img_np))\n",
        "\n",
        "# Ensure grayscale_cam is in the correct format (CV_8UC1)\n",
        "if len(grayscale_cam.shape) == 3:\n",
        "    grayscale_cam = grayscale_cam[:, :, 0] #If grayscale_cam has more than one channel, take the first one.\n",
        "grayscale_cam = np.uint8(255 * grayscale_cam)\n",
        "\n",
        "# Generate CAM\n",
        "cam_image = show_cam_on_image(img_np, grayscale_cam, use_rgb=True)"
      ],
      "metadata": {
        "id": "1AQk2tYzngsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the result\n",
        "plt.imshow(cam_image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2A7PZlIluT8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Display the original image and the Grad-CAM overlay\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np) # Changed image_np to img_np\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Grad-CAM')\n",
        "plt.imshow(cam_image)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DZrPH5zkbAhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Applying on diffrent data sample _ Infected"
      ],
      "metadata": {
        "id": "gsZzkymbzmwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model on a new image\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1 (13).jpg'  # Your test image\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "id": "w4sqA4J6y9Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate CAM\n",
        "targets = [ClassifierOutputTarget(1)]  # Assuming label 1 corresponds to 'infected'\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "\n",
        "# Resize the original image to match the CAM dimensions\n",
        "img = img.resize((224, 224))\n",
        "img_np = np.array(img) / 255.0\n",
        "\n",
        "# Ensure grayscale_cam has the correct number of channels\n",
        "if len(grayscale_cam.shape) == 2:\n",
        "    grayscale_cam = grayscale_cam[..., np.newaxis]"
      ],
      "metadata": {
        "id": "TyER5tl9y9Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate CAM\n",
        "cam_image = show_cam_on_image(img_np, grayscale_cam[0, :], use_rgb=True)\n",
        "\n",
        "# 8. Display the original image and the Grad-CAM overlay\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Grad-CAM')\n",
        "plt.imshow(cam_image)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HJ7Bki6Ly9Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Applying on diffrent data sample _ Infected"
      ],
      "metadata": {
        "id": "4UD8dN2uzboY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model on a new image\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1 (28).jpg'  # Your test image\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "# Generate CAM\n",
        "targets = [ClassifierOutputTarget(1)]  # Assuming label 1 corresponds to 'infected'\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "\n",
        "# Resize the original image to match the CAM dimensions\n",
        "img = img.resize((224, 224))\n",
        "img_np = np.array(img) / 255.0\n",
        "\n",
        "# Ensure grayscale_cam has the correct number of channels\n",
        "if len(grayscale_cam.shape) == 2:\n",
        "    grayscale_cam = grayscale_cam[..., np.newaxis]\n",
        "\n",
        "# Generate CAM\n",
        "cam_image = show_cam_on_image(img_np, grayscale_cam[0, :], use_rgb=True)\n",
        "\n",
        "# 8. Display the original image and the Grad-CAM overlay\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Grad-CAM')\n",
        "plt.imshow(cam_image)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1m4QSCH-zVBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ScoreCAM\n"
      ],
      "metadata": {
        "id": "3GKgOYhP_fHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-gradcam\n",
        "\n",
        "from pytorch_grad_cam import ScoreCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ... your existing code for model loading, transform, and device ...\n",
        "\n",
        "# Test the model on a new image\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1 (28).jpg'  # Your test image\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "# Initialize Score-CAM\n",
        "cam = ScoreCAM(model=model, target_layers=target_layers) # Replace model and target_layers\n",
        "\n",
        "# Generate CAM\n",
        "targets = [ClassifierOutputTarget(1)]  # Assuming label 1 corresponds to 'infected'\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "\n",
        "# Resize the original image to match the CAM dimensions\n",
        "img = img.resize((224, 224))\n",
        "img_np = np.array(img) / 255.0\n",
        "\n",
        "# Ensure grayscale_cam has the correct number of channels\n",
        "if len(grayscale_cam.shape) == 2:\n",
        "    grayscale_cam = grayscale_cam[..., np.newaxis]\n",
        "\n",
        "# Generate CAM\n",
        "cam_image = show_cam_on_image(img_np, grayscale_cam[0, :], use_rgb=True)\n",
        "\n",
        "# Display the original image and the Score-CAM overlay\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Score-CAM')\n",
        "plt.imshow(cam_image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o9yPKKVp0mC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GradCAMPlusPlus"
      ],
      "metadata": {
        "id": "QENoxzuM_q_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-gradcam\n",
        "\n",
        "from pytorch_grad_cam import GradCAMPlusPlus\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ... your existing code for model loading, transform, and device ...\n",
        "\n",
        "# Test the model on a new image\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1 (28).jpg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "# Initialize GradCAM++\n",
        "cam = GradCAMPlusPlus(model=model, target_layers=target_layers)\n",
        "\n",
        "# Generate CAM\n",
        "targets = [ClassifierOutputTarget(1)]\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "\n",
        "# ... (Resize, handle channels, and visualize as in the previous example) ...\n",
        "# Display the original\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Gradcam++')\n",
        "plt.imshow(cam_image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rvbKjaA41GUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XGradCAM"
      ],
      "metadata": {
        "id": "idwVt4sz_vDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_grad_cam import XGradCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "\n",
        "# Initialize XGrad-CAM\n",
        "cam = XGradCAM(model=model, target_layers=target_layers)\n",
        "\n",
        "# Generate the heatmap\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "\n",
        "# Visualize\n",
        "cam_image = show_cam_on_image(img_np, grayscale_cam[0], use_rgb=True)\n",
        "plt.imshow(cam_image)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('XGradCAM')\n",
        "plt.imshow(cam_image)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "i3KFZLHM1o_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EigenCAM"
      ],
      "metadata": {
        "id": "1kApDb4Q_yaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_grad_cam import EigenCAM\n",
        "\n",
        "# Initialize EigenCAM\n",
        "cam = EigenCAM(model=model, target_layers=target_layers)\n",
        "\n",
        "# Generate the heatmap\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "\n",
        "# Visualize\n",
        "cam_image = show_cam_on_image(img_np, grayscale_cam[0], use_rgb=True)\n",
        "plt.imshow(cam_image)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('EigenCAM')\n",
        "plt.imshow(cam_image)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zNQB8SSD18pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LayerCAM"
      ],
      "metadata": {
        "id": "b1RBf6lS_3jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_grad_cam import LayerCAM\n",
        "\n",
        "# Initialize LayerCAM\n",
        "cam = LayerCAM(model=model, target_layers=target_layers)\n",
        "\n",
        "# Generate the heatmap\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "\n",
        "# Visualize\n",
        "cam_image = show_cam_on_image(img_np, grayscale_cam[0], use_rgb=True)\n",
        "plt.imshow(cam_image)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('LayerCAM')\n",
        "plt.imshow(cam_image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pfgvzMBT29c1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Full-Gradient /  FullGrad"
      ],
      "metadata": {
        "id": "nNYUhqSx5zLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_grad_cam import FullGrad\n",
        "\n",
        "# Initialize Full-Gradient with target layers\n",
        "cam = FullGrad(model=model, target_layers=target_layers)\n",
        "\n",
        "# Generate the heatmap\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "\n",
        "# Visualize\n",
        "cam_image = show_cam_on_image(img_np, grayscale_cam[0], use_rgb=True)\n",
        "plt.imshow(cam_image)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Full-Gradient')\n",
        "plt.imshow(cam_image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uST3xDL75Xda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Thresholded CAM (Binary Mask)\n",
        "You can apply a threshold to the heatmap after generating it, converting the values into a binary mask (0/1). This highlights only the most activated regions."
      ],
      "metadata": {
        "id": "pSRhoyd36vFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate CAM (using any method, e.g., AblationCAM, GradCAM)\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "\n",
        "# Apply a threshold to binarize the heatmap (e.g., threshold at 0.5)\n",
        "threshold = 0.5\n",
        "binary_cam = (grayscale_cam[0] > threshold).astype(float)\n",
        "\n",
        "# Visualize the binary heatmap\n",
        "cam_image = show_cam_on_image(img_np, binary_cam, use_rgb=True)\n",
        "plt.imshow(cam_image)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Thresholded CAM ')\n",
        "plt.imshow(cam_image)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aHPdEm8r6Nbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lime import lime_image\n",
        "from skimage.segmentation import mark_boundaries\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the image\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1 (13).jpg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "img = img.resize((224, 224))  # Assuming your model expects 224x224 input\n",
        "img_np = np.array(img)\n",
        "\n",
        "# Convert the image to a format the model can accept\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "# Define a prediction function that returns model output probabilities\n",
        "def predict_fn(images):\n",
        "    # Convert images to tensors and pass through the model\n",
        "    inputs = [transform(Image.fromarray(image)).unsqueeze(0).to(device) for image in images]\n",
        "    inputs = torch.cat(inputs, dim=0)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs)  # Assuming your model variable is named `model`\n",
        "    return outputs.cpu().numpy()\n",
        "\n",
        "# Initialize LIME explainer\n",
        "explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "# Get LIME explanation\n",
        "explanation = explainer.explain_instance(img_np,\n",
        "                                         classifier_fn=predict_fn,\n",
        "                                         top_labels=2,\n",
        "                                         hide_color=0,\n",
        "                                         num_samples=1000)\n",
        "\n",
        "# Show explanation for class label 1 (infected)\n",
        "temp, mask = explanation.get_image_and_mask(label=1, positive_only=False, num_features=10, hide_rest=False)\n",
        "\n",
        "# Display the LIME result\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('LIME Explanation (Infected)')\n",
        "plt.imshow(mark_boundaries(temp, mask))\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oeV0qhqwJYHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OcclusionSensitivity"
      ],
      "metadata": {
        "id": "Jcd7LqaeACkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. SHAP (SHapley Additive exPlanations)\n",
        "SHAP values explain the output of a machine learning model by showing the contribution of each feature."
      ],
      "metadata": {
        "id": "Jbvyb_VzW6--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "# Preprocessing the image\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1 (13).jpg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "img = img.resize((224, 224))\n",
        "img_np = np.array(img) / 255.0\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "# SHAP Explainer\n",
        "explainer = shap.DeepExplainer(model, input_tensor)\n",
        "shap_values = explainer.shap_values(input_tensor)\n",
        "\n",
        "# Visualize SHAP explanation\n",
        "shap.image_plot(shap_values, np.array([img_np]))\n"
      ],
      "metadata": {
        "id": "mgXuedIR-bBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the image\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1 (13).jpg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "img = img.resize((224, 224))\n",
        "img_np = np.array(img) / 255.0\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "# Function to apply occlusion\n",
        "def apply_occlusion(image, size=32, stride=8):\n",
        "    heatmap = np.zeros((image.shape[1], image.shape[2]))\n",
        "    for y in range(0, image.shape[1] - size + 1, stride):\n",
        "        for x in range(0, image.shape[2] - size + 1, stride):\n",
        "            occluded_image = image.clone()\n",
        "            occluded_image[:, :, y:y+size, x:x+size] = 0  # Mask out a patch\n",
        "            output = model(occluded_image).squeeze()\n",
        "            heatmap[y:y+size, x:x+size] = output[1].item()  # Assuming label 1 is 'infected'\n",
        "    return heatmap\n",
        "\n",
        "# Generate occlusion heatmap\n",
        "occlusion_heatmap = apply_occlusion(input_tensor)\n",
        "\n",
        "# Visualize the occlusion sensitivity map\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Occlusion Sensitivity')\n",
        "plt.imshow(occlusion_heatmap, cmap='jet')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SML7Hs9WXA0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from captum.attr import IntegratedGradients\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the image\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1 (13).jpg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "img = img.resize((224, 224))\n",
        "img_np = np.array(img) / 255.0\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "# Integrated Gradients explainer\n",
        "ig = IntegratedGradients(model)\n",
        "attributions = ig.attribute(input_tensor, target=1)  # Assuming label 1 is 'infected'\n",
        "attributions = attributions.squeeze().cpu().detach().numpy()\n",
        "\n",
        "# Normalize attributions for visualization\n",
        "attributions = np.sum(np.abs(attributions), axis=0)\n",
        "attributions = (attributions - np.min(attributions)) / (np.max(attributions) - np.min(attributions))\n",
        "\n",
        "# Plot Integrated Gradients\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Integrated Gradients')\n",
        "plt.imshow(attributions, cmap='hot')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "waQloE2NXAwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to add noise to the image\n",
        "def add_noise(image, noise_level=0.1):\n",
        "    noise = noise_level * torch.randn_like(image).to(device)\n",
        "    return image + noise\n",
        "\n",
        "# Function to compute gradients with noise (SmoothGrad)\n",
        "def smoothgrad(input_tensor, n_samples=50, noise_level=0.1):\n",
        "    smoothed_grad = torch.zeros_like(input_tensor)\n",
        "    for i in range(n_samples):\n",
        "        noisy_input = add_noise(input_tensor, noise_level)\n",
        "        noisy_input.requires_grad = True\n",
        "        output = model(noisy_input).squeeze()[1]  # Assuming label 1 is 'infected'\n",
        "        output.backward()\n",
        "        smoothed_grad += noisy_input.grad.abs()\n",
        "    return smoothed_grad / n_samples\n",
        "\n",
        "# Load and preprocess the image\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1 (13).jpg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "img = img.resize((224, 224))\n",
        "img_np = np.array(img) / 255.0\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "# Generate SmoothGrad explanation\n",
        "smoothed_grad = smoothgrad(input_tensor).squeeze().cpu().detach().numpy()\n",
        "\n",
        "# Normalize for visualization\n",
        "smoothed_grad = np.sum(np.abs(smoothed_grad), axis=0)\n",
        "smoothed_grad = (smoothed_grad - np.min(smoothed_grad)) / (np.max(smoothed_grad) - np.min(smoothed_grad))\n",
        "\n",
        "# Plot SmoothGrad explanation\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('SmoothGrad')\n",
        "plt.imshow(smoothed_grad, cmap='hot')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YZqO5gSJXHJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "LIME: Perturbs the image to identify key features affecting the prediction.\n",
        "SHAP: Uses game theory to identify the contribution of individual features.\n",
        "Occlusion Sensitivity: Masks parts of the image to determine the importance of different regions.\n",
        "Integrated Gradients: Averages gradients over interpolated inputs for smoother attributions.\n",
        "SmoothGrad: Adds noise to inputs and averages gradients for smoother visualizations."
      ],
      "metadata": {
        "id": "_M2jans_XN9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some more latest XAI techniques beyond the traditional ones like GradCAM, LIME, and Integrated Gradients. These newer methods often provide more nuanced or application-specific insights into model behavior.\n",
        "\n",
        "1. Captum's Guided Backpropagation\n",
        "Guided Backpropagation modifies the standard backpropagation algorithm to suppress negative gradients, producing cleaner and sharper feature visualizations."
      ],
      "metadata": {
        "id": "WhMrbFJEXVpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import GuidedBackprop\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the image\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1 (13).jpg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "img = img.resize((224, 224))\n",
        "img_np = np.array(img) / 255.0\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "# Guided Backpropagation explainer\n",
        "guided_bp = GuidedBackprop(model)\n",
        "attributions = guided_bp.attribute(input_tensor, target=1)  # Assuming label 1 is 'infected'\n",
        "\n",
        "# Convert attributions to numpy\n",
        "attributions = attributions.squeeze().cpu().detach().numpy()\n",
        "attributions = np.sum(attributions, axis=0)  # Summing channels\n",
        "attributions = np.maximum(attributions, 0)  # ReLU to remove negative values\n",
        "attributions = (attributions - attributions.min()) / (attributions.max() - attributions.min())  # Normalize\n",
        "\n",
        "# Visualize Guided Backpropagation\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Guided Backpropagation')\n",
        "plt.imshow(attributions, cmap='hot')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t43xgzRvXJl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Score-Based Explainability (SBE)\n",
        "SBE assigns importance scores to input features by modeling how much each feature contributes to the prediction score. Unlike gradient-based methods, SBE computes scores directly from the model’s internal values without requiring backpropagation."
      ],
      "metadata": {
        "id": "OfrgZLoGXeXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the image\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1 (13).jpg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "img = img.resize((224, 224))\n",
        "img_np = np.array(img) / 255.0\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "# Forward pass and scoring\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    output = model(input_tensor)\n",
        "    output_score = output[0, 1].item()  # Get the score for 'infected' label\n",
        "\n",
        "# Create a simple scoring mechanism based on feature importance\n",
        "def score_features(input_tensor, score_function):\n",
        "    scores = np.zeros(input_tensor.shape[-2:])\n",
        "    for y in range(input_tensor.shape[-2]):\n",
        "        for x in range(input_tensor.shape[-1]):\n",
        "            perturbed_input = input_tensor.clone()\n",
        "            perturbed_input[:, :, y, x] = 0  # Zero out the pixel (perturbation)\n",
        "            perturbed_output = model(perturbed_input)[0, 1].item()\n",
        "            scores[y, x] = output_score - perturbed_output  # Score is the difference in output\n",
        "    return scores\n",
        "\n",
        "# Compute feature scores using SBE\n",
        "scores = score_features(input_tensor, output_score)\n",
        "\n",
        "# Normalize for visualization\n",
        "scores = (scores - scores.min()) / (scores.max() - scores.min())\n",
        "\n",
        "# Visualize SBE heatmap\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('SBE Heatmap')\n",
        "plt.imshow(scores, cmap='hot')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dnhcvY-RXZL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import DeepLift\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the image\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1 (13).jpg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "img = img.resize((224, 224))\n",
        "img_np = np.array(img) / 255.0\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "# DeepLIFT explainer\n",
        "deeplift = DeepLift(model)\n",
        "attributions = deeplift.attribute(input_tensor, target=1)  # Assuming label 1 is 'infected'\n",
        "\n",
        "# Normalize attributions for visualization\n",
        "attributions = attributions.squeeze().cpu().detach().numpy()\n",
        "attributions = np.sum(attributions, axis=0)  # Summing channels\n",
        "attributions = (attributions - np.min(attributions)) / (np.max(attributions) - np.min(attributions))\n",
        "\n",
        "# Visualize DeepLIFT attributions\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('DeepLIFT Attributions')\n",
        "plt.imshow(attributions, cmap='hot')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HSFLz6fiXhr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image, ImageFilter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the image\n",
        "image_path = '/content/drive/MyDrive/pocs/infected/1 (13).jpg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "img = img.resize((224, 224))\n",
        "img_np = np.array(img) / 255.0\n",
        "input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "# Function to apply perturbation (Gaussian Blur)\n",
        "def apply_perturbation(img_np, patch_size=32, stride=8):\n",
        "    perturbed_img = img.copy()\n",
        "    for y in range(0, img_np.shape[0] - patch_size + 1, stride):\n",
        "        for x in range(0, img_np.shape[1] - patch_size + 1, stride):\n",
        "            patch = img.crop((x, y, x + patch_size, y + patch_size))\n",
        "            blurred_patch = patch.filter(ImageFilter.GaussianBlur(radius=5))\n",
        "            perturbed_img.paste(blurred_patch, (x, y))\n",
        "    return np.array(perturbed_img) / 255.0\n",
        "\n",
        "# Generate Perturbation-Based Saliency Map\n",
        "perturbed_img_np = apply_perturbation(img_np)\n",
        "\n",
        "# Visualize Perturbation-Based Saliency Map\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(img_np)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Perturbation-Based Saliency Map')\n",
        "plt.imshow(perturbed_img_np)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GbtAaU0IXx-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of Advanced XAI Techniques:\n",
        "Guided Backpropagation: Generates sharp feature attributions by modifying backpropagation.\n",
        "DIME: Disentangles multi-modal data (like text and images) for separate interpretations.\n",
        "SBE: Directly assigns scores based on how important each feature is to the prediction.\n",
        "DeepLIFT: Provides faster, more efficient feature attributions than Integrated Gradients.\n",
        "TCAV: Focuses on testing concepts (like \"infected\" or \"healthy\") for their impact on predictions.\n",
        "PBSM: Introduces perturbations to the image to"
      ],
      "metadata": {
        "id": "y8s_q7lgX04T"
      }
    }
  ]
}